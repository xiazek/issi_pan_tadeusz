{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.489549Z",
     "start_time": "2025-11-23T23:16:29.487292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If ipytest isn't installed in your environment, run:\n",
    "# %pip install -q ipytest pytest"
   ],
   "id": "e151eb552be52be7",
   "outputs": [],
   "execution_count": 548
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## configuration",
   "id": "7135c6858b291b9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.497557Z",
     "start_time": "2025-11-23T23:16:29.495270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# \"Księga [numer księgi]\"\n",
    "CHAPTER_TITLE_REGEX_PL = r'^\\s*Księga\\s+(\\w+|[IVXLCDM\\d]+)\\s*$'\n",
    "\n",
    "DEFAULT_CONFIG_PL = {\n",
    "    'top_longest_words_number': 10,\n",
    "    'top_most_common_words_number': 10,\n",
    "    'ignored_words': [\n",
    "        'i', 'a', 'że', 'lub', \"w\", \"się\", \"z\", \"na\", \"nie\", \"jak\", \"do\", \"to\", \"o\", \"za\",\n",
    "        \"po\", \"co\", \"od\", \"lecz\", \"bo\", \"gdy\", \"ja\",\n",
    "    ],\n",
    "    'ignored_intro_lines': [\n",
    "        'Adam Mickiewicz',\n",
    "        'Pan Tadeusz',\n",
    "        'czyli ostatni zajazd na Litwie',\n",
    "        'ISBN 978-83-288-2495-9'\n",
    "    ],\n",
    "    'ignored_regexps': [\n",
    "        r'^.*?ISBN\\s+978-83-\\d{3}-\\d{4}-\\d{1,2}\\s*',\n",
    "        CHAPTER_TITLE_REGEX_PL\n",
    "    ]\n",
    "}\n",
    "\n",
    "# dla testowego pliku z teksrami piosenek [Chorus] [Verse 1] etc\n",
    "LYRICS_TAGS_REGEX = r'\\[.*?\\]'\n",
    "\n",
    "DEFAULT_CONFIG_EN = {\n",
    "    'top_longest_words_number': 10,\n",
    "    'top_most_common_words_number': 10,\n",
    "    'ignored_intro_lines': [],\n",
    "    'ignored_words': ['a', 'the', 'do'],\n",
    "    'ignored_regexps': [\n",
    "        LYRICS_TAGS_REGEX\n",
    "    ]\n",
    "}\n"
   ],
   "id": "af996cd5c153c75",
   "outputs": [],
   "execution_count": 549
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Technical functions (file loading, handling errors etc)",
   "id": "707e42b7be68bcd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.503927Z",
     "start_time": "2025-11-23T23:16:29.501734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_string(input_text):\n",
    "    \"\"\"Raises TypeError if input_text is not a string.\"\"\"\n",
    "    if not isinstance(input_text, str):\n",
    "        raise TypeError(\n",
    "            f\"expected a string, got {type(input_text).__name__}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_text(filename):\n",
    "    \"\"\"Loads text from a file.\"\"\"\n",
    "    ensure_string(filename)\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        raise ValueError(f\"Expected a .txt file, got {filename!r} instead.\")\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    return text"
   ],
   "id": "f523df2075fd152",
   "outputs": [],
   "execution_count": 550
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.510303Z",
     "start_time": "2025-11-23T23:16:29.507924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO remove me !!\n",
    "def tokenize_by_split(input_text):\n",
    "    \"\"\"Tokenizes text using string.split().\n",
    "    DO NOT USE IT.\n",
    "    This is a simple naive implementation that ignores punctuation etc\n",
    "    \"\"\"\n",
    "    ensure_string(input_text)\n",
    "    return input_text.split()\n",
    "\n",
    "\n",
    "print(tokenize_by_split('Ala ma kota, a kot ma Alę!'))"
   ],
   "id": "41788c9254c84bd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ala', 'ma', 'kota,', 'a', 'kot', 'ma', 'Alę!']\n"
     ]
    }
   ],
   "execution_count": 551
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.522630Z",
     "start_time": "2025-11-23T23:16:29.520672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize_by_regexp(input_text):\n",
    "    \"\"\"Tokenizes text using a regular expression to find word boundaries.\n",
    "    - Converts text to lowercase.\n",
    "    - Extracts word-like tokens (letters, numbers, underscores, digits).\n",
    "    - Raises TypeError for non-string inputs.\n",
    "    \"\"\"\n",
    "    ensure_string(input_text)\n",
    "    return re.findall(r\"\\b\\w+\\b\", input_text.lower())\n"
   ],
   "id": "1b09b8c71d4c07db",
   "outputs": [],
   "execution_count": 552
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.533962Z",
     "start_time": "2025-11-23T23:16:29.532082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def longest_words(input_text, n=10):\n",
    "    \"\"\"returns top n longest workds in input_text\n",
    "    it is using the build-in function sorted\n",
    "    \"\"\"\n",
    "    ensure_string(input_text)\n",
    "    all_words = tokenize_by_regexp(input_text)\n",
    "    words = sorted(all_words, key=len, reverse=True)[:n]\n",
    "    return words\n"
   ],
   "id": "8a7995d6876497cb",
   "outputs": [],
   "execution_count": 553
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.538072Z",
     "start_time": "2025-11-23T23:16:29.536321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def most_common_words(input_text, n=10):\n",
    "    \"\"\"returns top n most common workds in input_text\n",
    "    it is using the collections.Counter class\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    ensure_string(input_text)\n",
    "    all_words = tokenize_by_regexp(input_text)\n",
    "    word_counts = Counter(all_words)\n",
    "    most_common = word_counts.most_common(n)\n",
    "    return [word for word, count in most_common]"
   ],
   "id": "614648d53db110b4",
   "outputs": [],
   "execution_count": 554
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.543754Z",
     "start_time": "2025-11-23T23:16:29.540512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: remove me. not needed\n",
    "def most_common_words_in_text_file(filename, n=10):\n",
    "    \"\"\"returns top n most common words in filename from current folder\n",
    "    \"\"\"\n",
    "    return most_common_words(load_text(filename), n)\n",
    "\n",
    "\n",
    "most_common_words_in_text_file('one-more-cup-of-coffee.txt')"
   ],
   "id": "c02d2210c8d92ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your', 'the', 'to', 'of', 'is', 'one', 'more', 'cup', 'coffee', 'and']"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 555
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.549Z",
     "start_time": "2025-11-23T23:16:29.547179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: remove me. not needed\n",
    "def longest_words_in_text_file(filename, n=10):\n",
    "    \"\"\"returns top n longest workds in filename from current folder\n",
    "    \"\"\"\n",
    "    return longest_words(load_text(filename), n)\n"
   ],
   "id": "8aace0c3629128c0",
   "outputs": [],
   "execution_count": 556
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:29.563515Z",
     "start_time": "2025-11-23T23:16:29.561127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cleanup_text(text, config=DEFAULT_CONFIG_PL):\n",
    "    \"\"\"Removes words from text.\"\"\"\n",
    "    flags = re.IGNORECASE | re.DOTALL | re.MULTILINE\n",
    "    for regexp in config.get(\"ignored_regexps\", []):\n",
    "        text = re.sub(regexp, \"\", text, flags=flags)\n",
    "\n",
    "    # remove ignored common words\n",
    "    for word in config.get(\"ignored_words\", []):\n",
    "        regex = r'\\b' + re.escape(word) + r'\\b'\n",
    "        text = re.sub(regex, '', text, flags=re.IGNORECASE)\n",
    "    return text"
   ],
   "id": "af6305b81b6ba196",
   "outputs": [],
   "execution_count": 557
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.143428Z",
     "start_time": "2025-11-23T23:16:29.567683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ex01_imp():\n",
    "    cleaned_text = cleanup_text(load_text('pan-tadeusz.txt'))\n",
    "    print(\"Najdłuższe słowa w Panu Tadeuszu to:\")\n",
    "    print(', '.join(longest_words(cleaned_text)))\n",
    "\n",
    "    print(\"Najczęściej występujące słowa w Panu Tadeuszu to:\")\n",
    "    print(', '.join(most_common_words(cleaned_text)))\n",
    "\n",
    "\n",
    "ex01_imp()"
   ],
   "id": "a020a174f3ce1db5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najdłuższe słowa w Panu Tadeuszu to:\n",
      "niebezpieczeństwach, białopiotrowiczowi, najprzykładniejszy, niebezpieczeństwem, niebezpieczeństwem, niebezpieczeństwem, nierozstrzygniony, białopiotrowiczem, niebezpieczeństwo, niebezpieczeństwa\n",
      "Najczęściej występujące słowa w Panu Tadeuszu to:\n",
      "już, tak, pan, jest, ale, był, nim, rzekł, go, tylko\n"
     ]
    }
   ],
   "execution_count": 558
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unit Tests",
   "id": "9cd5e870e4f55b97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.160016Z",
     "start_time": "2025-11-23T23:16:55.158065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "ipytest.autoconfig()  # integrate pytest with the notebook"
   ],
   "id": "ee5f1a74548f26d9",
   "outputs": [],
   "execution_count": 559
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test longest_words",
   "id": "d18f884ffed4d5c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.166367Z",
     "start_time": "2025-11-23T23:16:55.163487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"text, n, expected\",\n",
    "    [\n",
    "        (\"Ala ma kota\", 1, [\"kota\"]),\n",
    "        (\"Ala ma kota\", 2, [\"kota\", \"ala\"]),\n",
    "        (\"Ala ma kota\", 3, [\"kota\", \"ala\", \"ma\"]),\n",
    "        (\"To be, or not to be, that is the question.\", 2, [\"question\", \"that\"])\n",
    "    ],\n",
    ")\n",
    "def test_longest_words(text, n, expected):\n",
    "    assert longest_words(text, n) == expected"
   ],
   "id": "d2e856e8658de064",
   "outputs": [],
   "execution_count": 560
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test tokenize_by_regexp",
   "id": "80ead133770bc962"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.172370Z",
     "start_time": "2025-11-23T23:16:55.169579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenize = tokenize_by_regexp\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"text, expected\",\n",
    "    [\n",
    "        (\"123 abc\", [\"123\", \"abc\"]),\n",
    "        # skipping interpunction\n",
    "        (\"Ala ma kota, kot ma Alę!\", [\"ala\", \"ma\", \"kota\", \"kot\", \"ma\", \"alę\"]),\n",
    "        # other witespace characters\n",
    "        (\"123 abc\\n456\\tdef\", [\"123\", \"abc\", \"456\", \"def\"]),\n",
    "        # edge cases\n",
    "        (\"\", []),\n",
    "        (\"   \\t\\n  \", []),\n",
    "    ],\n",
    ")\n",
    "def test_tokenize(text, expected):\n",
    "    assert tokenize(text) == expected"
   ],
   "id": "a841819af1a180ef",
   "outputs": [],
   "execution_count": 561
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test most_common_words",
   "id": "c2cd4340c72c194e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.178283Z",
     "start_time": "2025-11-23T23:16:55.175617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"text, n, expected\",\n",
    "    [\n",
    "        (\"world world world\", 1, ['world']),\n",
    "        (\"world world world\", 3, ['world']),\n",
    "        (\"world world world.\", 4, ['world']),\n",
    "        (\"World WORLD, worLD!\", 3, ['world']),\n",
    "        (\"Hello WORLD! Hello World, Hello Bob\", 3, ['hello', 'world', \"bob\"]),\n",
    "\n",
    "        (\"\", 3, []),\n",
    "    ]\n",
    ")\n",
    "def test_most_common_words(text, n, expected):\n",
    "    assert most_common_words(text, n) == expected"
   ],
   "id": "ba44a44010d7afa8",
   "outputs": [],
   "execution_count": 562
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.184129Z",
     "start_time": "2025-11-23T23:16:55.180788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_cleanup_text():\n",
    "    text = load_text('one-more-cup-of-coffee.txt')\n",
    "    config = DEFAULT_CONFIG_EN\n",
    "    cleaned_text = cleanup_text(text, config)\n",
    "\n",
    "    assert \"[Chorus]\" not in cleaned_text\n",
    "    assert \"[Verse 1]\" not in cleaned_text\n",
    "    assert \"[Verse 2]\" not in cleaned_text\n",
    "\n",
    "    # \"the\" is part of \"annother\"\n",
    "    assert \"another\" in cleaned_text\n",
    "    # the is removed as a separate word\n",
    "    assert \"the \" not in cleaned_text\n",
    "\n",
    "    assert \"One more cup of coffee for  road\" in cleaned_text"
   ],
   "id": "602fa76675e6767c",
   "outputs": [],
   "execution_count": 563
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:16:55.367464Z",
     "start_time": "2025-11-23T23:16:55.188510Z"
    }
   },
   "cell_type": "code",
   "source": "ipytest.run()",
   "id": "e2835ea818188c7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                                                             [100%]\u001B[0m\n",
      "\u001B[32m\u001B[32m\u001B[1m16 passed\u001B[0m\u001B[32m in 0.02s\u001B[0m\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 564
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
